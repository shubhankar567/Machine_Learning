{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97d22279-d7e0-412d-9eb2-282b7e84266f",
   "metadata": {},
   "source": [
    "# Answer 1:\n",
    "Min-Max scaling is a data preprocessing technique that is used to transform features by scaling each feature to a given range, usually between 0 and 1. It scales the values to a specific value range without changing the shape of the original distribution.\n",
    "\n",
    "The Min-Max scaling is done using the following formula:\n",
    "x_std = (x – x.min(axis=0)) / (x.max(axis=0) – x.min(axis=0))\n",
    "\n",
    "x_scaled = x_std * (max – min) + min\n",
    "\n",
    "Where min, max = feature_range.\n",
    "\n",
    "Here's an example of how Min-Max scaling can be applied using the `MinMaxScaler` class from the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b676ab9b-c5e0-46f1-a2f6-1fef837b6085",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.17647059]\n",
      " [0.25       0.        ]\n",
      " [0.5        0.52941176]\n",
      " [1.         1.        ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "data = [[-1, 4], [-0.5, 1], [0, 10], [1, 18]]\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "print(scaler.fit_transform(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "292925bf-91f1-4335-bf89-44ab691502f0",
   "metadata": {},
   "source": [
    "# Answer 2:\n",
    "Unit Vector technique, also known as Normalization, is a feature scaling method that scales the values of a feature vector so that it has a magnitude of 1. This is achieved by dividing each observation vector by either the Manhattan distance (l1 norm) or the Euclidean distance (l2 norm) of the vector ³.\n",
    "\n",
    "The main difference between Min-Max scaling and Unit Vector technique is that Min-Max scaling shrinks the data within a given range, usually between 0 and 1, while Unit Vector technique scales the values of a feature vector so that it has a magnitude of 1 ⁴.\n",
    "\n",
    "Here's an example of how Unit Vector technique can be applied using the `Normalizer` class from the `sklearn.preprocessing` module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5cc534b1-c19f-4e58-bc03-1afe9b3fa05c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.4472136   0.89442719]\n",
      " [-0.08304548  0.99654576]\n",
      " [ 0.          1.        ]\n",
      " [ 0.05547002  0.99846035]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import Normalizer\n",
    "data = [[-1, 2], [-0.5, 6], [0, 10], [1, 18]]\n",
    "scaler = Normalizer()\n",
    "print(scaler.fit_transform(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab09591-6c09-49eb-aec4-fd43c6b1a8ee",
   "metadata": {},
   "source": [
    "# Answer 3:\n",
    "Principal Component Analysis (PCA) is a dimensionality reduction method that is often used to reduce the dimensionality of large datasets by transforming a large set of variables into a smaller one that still contains most of the information in the large set ².\n",
    "\n",
    "PCA works by projecting the data onto a set of orthogonal axes, where each axis represents a principal component that captures the variance in the data ³. The first principal component captures the most variance, while each subsequent component captures less variance.\n",
    "\n",
    "Here's an example of how PCA can be applied using the `PCA` class from the `sklearn.decomposition` module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "51bdafe8-cb78-4e75-a47e-5adfd026bd50",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.datasets import make_classification\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_informative=15, random_state=0)\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f669c6-ccf9-4781-beb0-ec159392e76d",
   "metadata": {},
   "source": [
    "This will create a synthetic binary classification problem with 1,000 examples and 20 input features, 15 inputs of which are meaningful. Then, it will apply PCA to reduce the dimensionality of the data from 20 to 2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ec7c34-8e04-4981-b948-8876cb099e3f",
   "metadata": {},
   "source": [
    "# Answer 4:\n",
    "PCA can be used for feature extraction by creating new features from the principal components. The scores of the principal components can be used as inputs for other machine learning models, such as regression or classification ³. This can improve the performance and generalization of the models, since the new features are less noisy and more informative ³.\n",
    "\n",
    "Here's an example of how PCA can be used for feature extraction:\n",
    "```python\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_informative=15, random_state=0)\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X)\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X_pca, y)\n",
    "```\n",
    "This will create a synthetic binary classification problem with 1,000 examples and 20 input features, 15 inputs of which are meaningful. Then, it will apply PCA to reduce the dimensionality of the data from 20 to 2 and use the scores of the principal components as inputs for a logistic regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5378deee-0e67-44ce-b873-61f38e833d9c",
   "metadata": {},
   "source": [
    "# Answer 5:\n",
    "In building a recommendation system for a food delivery service, Min-Max scaling can be used to preprocess the data by scaling the values of the features such as price, rating, and delivery time to a given range, usually between 0 and 1. This is done to ensure that each feature contributes equally to the analysis and that the model is not biased towards features with larger ranges.\n",
    "\n",
    "The Min-Max scaling is done using the following formula:\n",
    "x_std = (x – x.min(axis=0)) / (x.max(axis=0) – x.min(axis=0))\n",
    "\n",
    "x_scaled = x_std * (max – min) + min\n",
    "\n",
    "where min, max = feature_range.\n",
    "\n",
    "Here's an example of how Min-Max scaling can be applied using the `MinMaxScaler` class from the `sklearn.preprocessing` module:\n",
    "```python\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "data = [[10, 4.5, 30], [15, 3.5, 45], [12, 5.0, 20], [8, 4.0, 60]]\n",
    "scaler = MinMaxScaler()\n",
    "data_scaled = scaler.fit_transform(data)\n",
    "```\n",
    "This will scale the values of the price, rating, and delivery time features to a range between 0 and 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f85fcadc-1e37-442c-8f48-ebea158f4f03",
   "metadata": {},
   "source": [
    "# Answer 6:\n",
    "In building a model to predict stock prices, PCA can be used to reduce the dimensionality of the dataset by transforming a large set of features, such as company financial data and market trends, into a smaller set of principal components that still contains most of the information in the large set.\n",
    "\n",
    "PCA works by projecting the data onto a set of orthogonal axes, where each axis represents a principal component that captures the variance in the data. The first principal component captures the most variance, while each subsequent component captures less variance.\n",
    "\n",
    "Here's an example of how PCA can be applied using the `PCA` class from the `sklearn.decomposition` module:\n",
    "```python\n",
    "from sklearn.decomposition import PCA\n",
    "data = [[...], [...], ...] # dataset with many features\n",
    "pca = PCA(n_components=2)\n",
    "data_pca = pca.fit_transform(data)\n",
    "```\n",
    "This will apply PCA to reduce the dimensionality of the data from many features to 2 principal components.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5347b6c5-dde1-47b8-ae51-98db65a62177",
   "metadata": {},
   "source": [
    "# Answer 7:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9b66e6da-1972-4d76-9071-43f151b6ed85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.        ]\n",
      " [-0.57894737]\n",
      " [-0.05263158]\n",
      " [ 0.47368421]\n",
      " [ 1.        ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "data = np.array([1, 5, 10, 15, 20]).reshape(-1, 1)\n",
    "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "data_scaled = scaler.fit_transform(data)\n",
    "print(data_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "363b6dba-8b4d-419a-b6fe-e7909cc2f024",
   "metadata": {},
   "source": [
    "# Answer 8:\n",
    "To perform Feature Extraction using PCA on a dataset containing the features [height, weight, age, gender, blood pressure], we can use the `PCA` class from the `sklearn.decomposition` module. However, without knowing the specific values of the dataset and their variance, it is not possible to determine how many principal components should be retained.\n",
    "\n",
    "The number of principal components to retain depends on the amount of variance that needs to be captured by the model. Typically, enough principal components are chosen to capture around 95% of the variance in the data. This can be done by examining the explained variance ratio of each principal component and choosing enough components to reach the desired threshold.\n",
    "\n",
    "Here's an example of how PCA can be applied using the `PCA` class from the `sklearn.decomposition` module:\n",
    "```python\n",
    "from sklearn.decomposition import PCA\n",
    "data = [[...], [...], ...] # dataset with features [height, weight, age, gender, blood pressure]\n",
    "pca = PCA()\n",
    "data_pca = pca.fit_transform(data)\n",
    "```\n",
    "This will apply PCA to the data and transform it into principal components. The number of principal components to retain can then be determined by examining the explained variance ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff3b617-c1c2-4cae-8ebd-1cd6e0f6031d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
