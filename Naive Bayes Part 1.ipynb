{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3158be47-4e0f-4885-aa0d-1f311a4e4adf",
   "metadata": {},
   "source": [
    "# Answer 1:\n",
    "Bayes' theorem is a mathematical formula for determining conditional probability. It is named after the 18th-century British mathematician Thomas Bayes. In probability theory and statistics, Bayes' theorem describes the probability of an event, based on prior knowledge of conditions that might be related to the event. It is stated mathematically as the following equation: $P(A|B) = \\frac{P(B|A) * P(A)}{ P(B)}$ where `A` and `B` are events and `P(A|B)` is a conditional probability: the probability of event `A` occurring given that `B` is true. One of the many applications of Bayes' theorem is Bayesian inference, a particular approach to statistical inference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f52526c-a339-498b-9050-749e8f46ee91",
   "metadata": {},
   "source": [
    "# Answer 2;\n",
    "It is stated mathematically as the following equation: $P(A/B) = \\frac{P(B/A) \\  P(A)}{ P(B)}$ where `A` and `B` are events and `P(A|B)` is a conditional probability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f674d077-feab-4a6d-89c2-d3ccbd4d5140",
   "metadata": {},
   "source": [
    "# Answer 3:\n",
    "Bayes' theorem has many practical applications in various fields such as mathematics, medicine, finance, marketing, and engineering. For example, in medicine, Bayes' theorem can be used to determine the accuracy of medical test results by taking into consideration how likely any given person is to have a disease and the general accuracy of the test. \n",
    "\n",
    "Another application of Bayes' theorem is Bayesian inference, which is a particular approach to statistical inference. Bayesian inference is fundamental to Bayesian statistics and is used in many data science algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb3e220-4a9a-4c47-ad72-431d7511a8fa",
   "metadata": {},
   "source": [
    "# Answer 4:\n",
    "Bayes' theorem is a mathematical formula for determining conditional probability. \n",
    "\n",
    "Conditional probability is the probability of an event occurring given that another event has already occurred. Bayes' theorem describes the relationship between two events and their associated conditional probabilities. \n",
    "\n",
    "It is stated mathematically as the following equation: `P(A|B) = (P(B|A) * P(A)) / P(B)` where `A` and `B` are events and `P(A|B)` is a conditional probability: the probability of event `A` occurring given that `B` is true. In other words, Bayes' theorem provides a way to calculate the probability of an event based on new information that is related to that event."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bdc6b14-c035-4fd2-b958-100dca80d60b",
   "metadata": {},
   "source": [
    "# Answer 5:\n",
    "There are several types of Naive Bayes classifiers, including Gaussian Naive Bayes, Multinomial Naive Bayes, and Bernoulli Naive Bayes. The choice of which type of Naive Bayes classifier to use depends on the nature of the data and the problem at hand. \n",
    "\n",
    "For example, if the data is continuous and follows a normal distribution, then Gaussian Naive Bayes may be a good choice. \n",
    "\n",
    "If the data is discrete and represents counts or frequencies, then Multinomial Naive Bayes may be more appropriate. \n",
    "\n",
    "If the data is binary, then Bernoulli Naive Bayes may be the best choice. It's important to carefully analyze the data and understand its characteristics before choosing a specific type of Naive Bayes classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d0c556-11ba-4979-81e4-9ecf1e0423cc",
   "metadata": {},
   "source": [
    "# Answer 6:\n",
    "To classify the new instance with features X1 = 3 and X2 = 4 using Naive Bayes, we need to calculate the posterior probabilities for each class, given the observed feature values. Assuming equal prior probabilities for each class, we can use the following formula to calculate the posterior probabilities: `P(class | X1=3, X2=4) = P(X1=3 | class) * P(X2=4 | class) * P(class) / P(X1=3, X2=4)`.\n",
    "\n",
    "Since we are only interested in the relative probabilities of each class, we can ignore the denominator `P(X1=3, X2=4)` as it is constant for both classes. We can calculate the probabilities `P(X1=3 | class)` and `P(X2=4 | class)` using the frequency table provided in your previous message.\n",
    "\n",
    "For class A, we have `P(X1=3 | A) = 4 / (3 + 3 + 4) = 0.4` and `P(X2=4 | A) = 3 / (4 + 3 + 3 + 3) = 0.231`. Therefore, `P(A | X1=3, X2=4) = P(X1=3 | A) * P(X2=4 | A) * P(A) = 0.4 * 0.231 * 0.5 = 0.0462`.\n",
    "\n",
    "For class B, we have `P(X1=3 | B) = 1 / (2 + 2 + 1) = 0.2` and `P(X2=4 | B) = 3 / (2 + 2 + 2 + 3) = 0.333`. Therefore, `P(B | X1=3, X2=4) = P(X1=3 | B) * P(X2=4 | B) * P(B) = 0.2 * 0.333 * 0.5 = 0.03333`.\n",
    "\n",
    "Since `P(A | X1=3, X2=4)` is greater than `P(B | X1=3, X2=4)`, Naive Bayes would predict that the new instance with features X1 = 3 and X2 = 4 belongs to **class A**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d3f93c-61a3-472a-8581-7e6acd8ef0b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
