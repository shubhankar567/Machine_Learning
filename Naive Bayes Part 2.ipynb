{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75136973-7d20-44bd-8586-e35d9e51a8e1",
   "metadata": {},
   "source": [
    "# Answer 1:\n",
    "Let's denote the event that an employee uses the company's health insurance plan as `H`, and the event that an employee is a smoker as `S`. According to the problem statement, we know that `P(H) = 0.7` and `P(S | H) = 0.4`. The probability that an employee is a smoker given that he/she uses the health insurance plan is `P(S | H)`, which is equal to `0.4`. So, the probability that an employee is a smoker given that he/she uses the health insurance plan is `40%`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16289ca5-fb84-4c93-a6c5-f224bf3e6bf0",
   "metadata": {},
   "source": [
    "# Answer 2:\n",
    "Bernoulli Naive Bayes and Multinomial Naive Bayes are two types of Naive Bayes classifiers that are used for different types of data. Bernoulli Naive Bayes is used for discrete data, where features are only in binary form. It models the presence or absence of a feature in a given class. On the other hand, Multinomial Naive Bayes is widely used for document classification, where it keeps the count of frequent words present in the documents. It models the number of counts of a feature in a given class. In summary, Bernoulli Naive Bayes is suitable for binary data, while Multinomial Naive Bayes is suitable for count data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc0fc170-d3e5-4c83-a191-de6635f51c19",
   "metadata": {},
   "source": [
    "# Answer 3:\n",
    "When dealing with missing values in a Bernoulli Naive Bayes classifier, one approach is to simply ignore the missing values and exclude them from the probability calculations. This means that when calculating the probabilities for a given class, only the non-missing feature values are taken into account. Another approach is to impute the missing values with some other placeholder value or an average value corresponding to the overall dataset distribution. However, this approach may not always be appropriate, as it can introduce bias into the model. Ultimately, the choice of how to handle missing values in a Bernoulli Naive Bayes classifier depends on the specific problem and dataset at hand."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31446367-9704-42b0-b751-82a2a244779c",
   "metadata": {},
   "source": [
    "# Answer 4:\n",
    "Yes, Gaussian Naive Bayes can be used for multi-class classification. In a multi-class classification problem, there are more than two possible classes for the target variable. Gaussian Naive Bayes can be used to calculate the probabilities of each class given the observed feature values, and the class with the highest probability is chosen as the prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a87751b3-1730-4e28-87a6-9e1c37eb277e",
   "metadata": {},
   "source": [
    "# Answer 5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "394f7684-af8c-4c33-9bc9-ce4bd3fa1ac3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating BernoulliNB...\n",
      "Accuracy: 0.884\n",
      "Precision: 0.887\n",
      "Recall: 0.815\n",
      "F1 score: 0.848\n",
      "\n",
      "Evaluating MultinomialNB...\n",
      "Accuracy: 0.786\n",
      "Precision: 0.739\n",
      "Recall: 0.721\n",
      "F1 score: 0.728\n",
      "\n",
      "Evaluating GaussianNB...\n",
      "Accuracy: 0.822\n",
      "Precision: 0.710\n",
      "Recall: 0.957\n",
      "F1 score: 0.813\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB, GaussianNB\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/spambase/spambase.data', header=None)\n",
    "X = data.iloc[:, :-1]\n",
    "y = data.iloc[:, -1]\n",
    "\n",
    "# Define the classifiers\n",
    "classifiers = {\n",
    "    'BernoulliNB': BernoulliNB(),\n",
    "    'MultinomialNB': MultinomialNB(),\n",
    "    'GaussianNB': GaussianNB()\n",
    "}\n",
    "\n",
    "# Define the performance metrics\n",
    "scoring = ['accuracy', 'precision', 'recall', 'f1']\n",
    "\n",
    "# Evaluate the classifiers using 10-fold cross-validation\n",
    "for name, clf in classifiers.items():\n",
    "    print(f'Evaluating {name}...')\n",
    "    scores = cross_validate(clf, X, y, cv=10, scoring=scoring)\n",
    "    print(f'Accuracy: {scores[\"test_accuracy\"].mean():.3f}')\n",
    "    print(f'Precision: {scores[\"test_precision\"].mean():.3f}')\n",
    "    print(f'Recall: {scores[\"test_recall\"].mean():.3f}')\n",
    "    print(f'F1 score: {scores[\"test_f1\"].mean():.3f}')\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34302b83-ce3a-402d-afb0-068c9c163d3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8d5396e2-6624-4d8a-b241-bda41505e1d1",
   "metadata": {},
   "source": [
    "In case of Spam, we have to reduce the False Positive predictions. We have to, thus, increase our `precision value`.\n",
    "Bernoulli NB provides high accuracy as well as precision score. So, `BernoulliNB` will be the best model for this problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7949795f-157c-4e56-a872-c323a8f5a90b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
