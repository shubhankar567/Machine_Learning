{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b6bf3c2-5e88-48f0-995f-ace1bb401ecf",
   "metadata": {},
   "source": [
    "# Answer 1:\n",
    "Ridge Regression is a technique used when the data suffers from multicollinearity (independent variables are highly correlated). In multicollinearity, even though the least squares estimates (OLS) are unbiased, their variances are large which deviates the observed value far from the true value. Ridge regression adds a **shrinkage penalty** to the least squares method, which minimizes the sum of squared residuals (RSS) plus a penalty term in an additive way. The advantage of ridge regression compared to least squares regression lies in the **bias-variance tradeoff**. The basic idea of ridge regression is to introduce a little bias so that the variance can be substantially reduced, which leads to a lower overall mean squared error (MSE)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fbe6ea6-53c8-45fb-bbd6-4ff6864dbd32",
   "metadata": {},
   "source": [
    "# Answer 2:\n",
    "The assumptions of Ridge Regression are the same as that of linear regression: **linearity**, **constant variance**, and **independence**. However, Ridge Regression does not provide confidence limits, so the distribution of errors to be normal need not be assumed. Ridge Regression assumes that there is a **linear relationship** between the independent variables and the dependent variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d14920-d70a-4c59-b36a-0fb7176de442",
   "metadata": {},
   "source": [
    "# Answer 3:\n",
    "In Ridge Regression, the value of the tuning parameter lambda is chosen using **cross-validation**. The idea is to make the fit small by making the residual sum of squares small plus adding a shrinkage penalty. The best lambda for your data can be defined as the lambda that minimizes the cross-validation prediction error rate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb20721-0275-4e2c-83e6-2c4b83438d1b",
   "metadata": {},
   "source": [
    "# Answer 4:\n",
    "Ridge Regression can select features. It uses a penalty applied to the sum of the squares of all regression coefficients. Without wasting any information about predictions it tries to determine variables that have exactly zero effects. So the multicollinear variables can be removed using Ridge regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f48097-1015-42b8-a630-a7d2ad0bba72",
   "metadata": {},
   "source": [
    "# Answer 5:\n",
    "Ridge Regression is a technique used to eliminate multicollinearity in data models. In a case where observations are fewer than predictor variables, ridge regression is the most appropriate technique. Ridge regression is the method used for the analysis of multicollinearity in multiple regression data. It is most suitable when a data set contains a higher number of predictor variables than the number of observations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "802128eb-af31-4bd9-a72b-feabcc37d915",
   "metadata": {},
   "source": [
    "# Answer 6:\n",
    "Yes, Ridge Regression can handle both categorical and continuous independent variables. Categorical variables can be encoded using dummy variables or one-hot encoding, where each category is represented by a binary variable. Ridge Regression is essentially using a Tikhonov regularized version of the covariance matrix of X, where X is the design matrix containing both categorical and continuous independent variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1070ce60-4ca3-40ca-8a88-be58b90e7c76",
   "metadata": {},
   "source": [
    "# Answer 7:\n",
    "In Ridge Regression, the coefficients are defined as a response vector y and a predictor matrix X. The coefficient value signifies how much the mean of the dependent variable changes given a one-unit shift in the independent variable while holding other variables in the model constant. The turning factor λ controls the strength of the penalty term. If λ = 0, the objective becomes similar to simple linear regression, and we get the same coefficients as simple linear regression.\n",
    "\n",
    "However, it is important to note that it can be difficult to interpret the coefficients in Ridge Regression since they get shrunk towards zero."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c4fdf67-dc30-4103-9d95-220cea03b5c9",
   "metadata": {},
   "source": [
    "# Answer 8:\n",
    "Yes, Ridge Regression can be used for time-series data analysis. In time series regression, the dependent variable is a time series, and the independent variables can be other time series or non-time series variables. Time series regression helps you understand the relationship between variables over time and forecast future values of the dependent variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a6a34c-2ea1-4dee-a746-4376ad9d355f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
