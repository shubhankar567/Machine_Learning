{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fdba39af-b47e-4834-a973-dc65d4a33004",
   "metadata": {},
   "source": [
    "# Answer 1:\n",
    "Linear regression and logistic regression are both supervised machine learning algorithms that serve different purposes. Linear regression is used for predicting continuous values, while logistic regression is used for binary classification of values.\n",
    "\n",
    "An example of a scenario where logistic regression would be more appropriate is when a college admissions officer wants to use predictor variables such as GPA and ACT score to predict the probability that a student will get accepted into a certain university. In this scenario, logistic regression would be used because the response variable is categorical and can only take on two values â€“ accepted or not accepted."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f09dbce0-04c7-4f31-a84c-192414067f8e",
   "metadata": {},
   "source": [
    "# Answer 2:\n",
    "The cost function used in logistic regression is also known as the cross-entropy or log loss. It is a convex function with a single global optimum. The cost function can be optimized using an iterative optimization algorithm such as gradient descent.\n",
    "\n",
    "$J(\\theta) = \\frac{1}{1 + e^{\\theta_0 + \\theta_1 x + ... + \\theta_n x_n}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc6f51e6-458a-4de0-afa8-6f38d710217d",
   "metadata": {},
   "source": [
    "# Answer 3:\n",
    "Regularization is a technique used to prevent overfitting in logistic regression modeling. Without regularization, the asymptotic nature of logistic regression would keep driving loss towards 0 in high dimensions. Regularization works by adding a penalty term to the loss function, which encourages the model to have smaller coefficients, thus reducing its complexity and preventing overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c1813e4-b3a5-424a-840f-e9971911210e",
   "metadata": {},
   "source": [
    "# Answer 4:\n",
    "The ROC (Receiver Operating Characteristic) curve is a plot that displays the sensitivity (the ability of the model to predict an event correctly) versus 1-specificity for the possible cut-off classification probability values in a logistic regression model. The ROC curve is used to determine the best cutoff value for predicting whether a new observation is a \"failure\" (0) or a \"success\" (1).\n",
    "\n",
    "The more that the ROC curve hugs the top left corner of the plot, the better the model does at classifying the data into categories. To quantify this, we can calculate the AUC (Area Under the Curve) which tells us how much of the plot is located under the curve. The closer AUC is to 1, the better the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "483fa966-a6aa-4b69-a829-7c20ab0b6df3",
   "metadata": {},
   "source": [
    "# Answer 5:\n",
    "There are several techniques that can be used for feature selection in logistic regression. One common method is Recursive Feature Elimination (RFE), which involves training a model on a subset of the features, and then iteratively removing the least important features one by one until we are left with the desired number of features. Another method is to use a wrapper approach, which involves using a search algorithm to find the best combination of features for a predictive model.\n",
    "\n",
    "These techniques help improve the model's performance by selecting only the most relevant features, thus reducing the model's complexity and improving its ability to generalize to new data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a212fea8-8c5f-4fde-ae0f-0ccaa08df9fc",
   "metadata": {},
   "source": [
    "# Answer 6:\n",
    "Imbalanced datasets can be challenging to handle in logistic regression. There are several strategies for dealing with class imbalance, including subsampling the negative set to reduce it to be the same size as the positive set, using weighted logistic regression, or using the class_weight option and setting the balanced value.\n",
    "\n",
    "Another approach is to use resampling techniques such as undersampling, which involves eliminating or deleting data points of the majority class to make an equal ratio of major and minor classes.\n",
    "\n",
    "These techniques help improve the model's performance by balancing the data, thus reducing bias and improving its ability to generalize to new data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b0eca77-8ebe-4e98-a47b-3b3c6eb6b576",
   "metadata": {},
   "source": [
    "# Answer 7:\n",
    "There are several common issues and challenges that may arise when implementing logistic regression. One such issue is multicollinearity among the independent variables, which can lead to unstable estimates of the coefficients and make it difficult to interpret the results of the model.\n",
    "\n",
    "One way to address multicollinearity is to use variable selection techniques to remove highly correlated variables from the model. Another approach is to use regularization techniques such as ridge regression, which can help to stabilize the estimates of the coefficients.\n",
    "\n",
    "Other challenges of logistic regression include difficulty in interpreting the results, difficulty in capturing complex relationships, and sensitivity to outliers. These challenges can be addressed by using techniques such as regularization, feature selection, and robust regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "266f6f0a-3d72-42a7-b282-3fcee5e8750b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
