{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd86e0f1-2f18-4987-9f80-dd74f1456861",
   "metadata": {},
   "source": [
    "# Answer 1:\n",
    "The k-nearest neighbors algorithm, also known as KNN or k-NN, is a non-parametric supervised learning method first developed by Evelyn Fix and Joseph Hodges in 1951, and later expanded by Thomas Cover. It is used for classification and regression. In both cases, the input consists of the k closest training examples in a data set. The output depends on whether k-NN is used for classification or regression: \n",
    "- In k-NN classification, the output is a class membership. An object is classified by a plurality vote of its neighbors, with the object being assigned to the class most common among its k nearest neighbors (k is a positive integer, typically small).\n",
    "- In k-NN regression, the output is the property value for the object. This value is the average of the values of k nearest neighbors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72aac866-030b-486c-a3f4-8a343c8bb5a5",
   "metadata": {},
   "source": [
    "# Answer 2:\n",
    "Choosing the value of k in the k-nearest neighbors (k-NN) algorithm is an important step and can affect the accuracy of the model. There is no one-size-fits-all answer to this question, as the optimal value of k depends on the specific data set and problem at hand. Here are some common techniques for choosing the value of k:\n",
    "- **Cross-validation**: One way to determine the best value of k is to use cross-validation. This involves dividing the data into training and validation sets, training the model on the training set for different values of k, and evaluating its performance on the validation set. The value of k that results in the best performance on the validation set is chosen as the optimal value.\n",
    "- **The square root rule**: A simple rule of thumb for choosing k is to set it equal to the square root of the number of samples in the training dataset.\n",
    "- **Avoiding overfitting**: It is generally not recommended to choose a very small value for k, as this can lead to overfitting. Overfitting occurs when the model is too complex and fits the training data too well, including its noise and errors. This can result in poor generalization to new data.\n",
    "\n",
    "In summary, there is no definitive answer to how to choose the value of k in k-NN, and it often requires some experimentation and fine-tuning to find the optimal value for a given problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d2db25-17f9-4b87-9be9-9efde9a600c3",
   "metadata": {},
   "source": [
    "# Answer 3:\n",
    "The key difference between KNN classifier and KNN regressor is that KNN regressor tries to predict the value of the output variable by using a local average, while KNN classification attempts to predict the class to which the output variable belongs by computing the local probability. KNN Classifier is used for classification problems and KNN regression is used for solving regression problems. We do a classification when the response variable is a factor with levels, and we classify the response into levels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d591d27-ed76-4e71-b049-d3ceff63063c",
   "metadata": {},
   "source": [
    "# Answer 4:\n",
    "The performance of the KNN algorithm can be measured using various metrics. Some of the most popular metrics used to evaluate the performance of KNN classifier are:\n",
    "- **Accuracy**: It is the most intuitive performance measure, and defined as the ratio of the number of correctly classified data objects to the total number of data objects.\n",
    "- **Precision**: It is simply a ratio of correctly predicted positive data objects to the total predicted positive data objects.\n",
    "- **Recall**: It is a ratio of correctly predicted positive data objects to all the data objects in actual class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b6fec2-ef5c-4942-ae06-516e3c7730c0",
   "metadata": {},
   "source": [
    "# Answer 5:\n",
    "The curse of dimensionality is a phenomenon where the feature space becomes increasingly sparse for an increasing number of dimensions of a fixed-size training dataset. KNN is very susceptible to overfitting due to the curse of dimensionality. KNN performs best with a low number of features, and when the number of features increases, it requires more data. This creates an overfitting problem because no one knows which piece of noise will contribute to the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82369e82-2ae6-43a7-876f-5ad53f1c0084",
   "metadata": {},
   "source": [
    "# Answer 6:\n",
    "To handle missing values in KNN, you can use the **KNNImputer** from **sklearn.impute**. Here's how it works:\n",
    "1. Load KNNImputer: `from sklearn.impute import KNNImputer`\n",
    "2. Initialize KNNImputer: You can define your own `n_neighbors` value (as its typical of KNN algorithm). `imputer = KNNImputer(n_neighbors=2)`\n",
    "3. Impute/Fill Missing Values: Each sampleâ€™s missing values are imputed using the mean value from `n_neighbors` nearest neighbors found in the training set. Two samples are close if the features that neither is missing are close. By default, a euclidean distance metric that supports missing values, `nan_euclidean_distances`, is used to find the nearest neighbors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f90d95fa-7cdb-453f-8e07-629a933ca8b4",
   "metadata": {},
   "source": [
    "# Answer 7:\n",
    "The KNN algorithm can be used for both classification and regression problems. The choice between using a KNN classifier or a KNN regressor depends on the type of problem you are trying to solve. KNN classifier is used for classification problems where the output variable is a categorical variable, while KNN regressor is used for regression problems where the output variable is a continuous variable.\n",
    "\n",
    "In terms of performance, both KNN classifier and KNN regressor can perform well when the data is well-behaved and the number of features is low. However, as the number of features increases, the performance of both algorithms can degrade due to the curse of dimensionality. The choice between using a KNN classifier or a KNN regressor ultimately depends on the type of problem you are trying to solve and the nature of your data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50adb673-8fc7-494c-a264-e74ec0d91a2e",
   "metadata": {},
   "source": [
    "# Answer 8:\n",
    "The KNN algorithm is a simple but powerful machine learning algorithm that can be used for both classification and regression tasks. Some of the strengths of the KNN algorithm include its simplicity, flexibility, and robustness to noisy data. However, the KNN algorithm also has several weaknesses that can affect its performance. Some of the weaknesses of the KNN algorithm include its sensitivity to the choice of the number of neighbors (K), its high computational cost when working with large datasets, and its susceptibility to the curse of dimensionality.\n",
    "\n",
    "To address these weaknesses, several modified versions of the KNN algorithm have been developed. These variants aim to remove the weaknesses of the KNN algorithm and provide a more efficient method. For example, one approach to addressing the curse of dimensionality is to use dimensionality reduction techniques such as Principal Component Analysis (PCA) or Linear Discriminant Analysis (LDA) to reduce the number of features before applying the KNN algorithm. Another approach is to use feature selection techniques to select a subset of relevant features that can improve the performance of the KNN algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33413c9f-0f2c-4932-9a40-a80e39f2393b",
   "metadata": {},
   "source": [
    "# Answer 9:\n",
    "Euclidean distance and Manhattan distance are two ways of measuring the distance between two points. Euclidean distance is based on squared error distance, whereas Manhattan distance is based on absolute value distance. Euclidean distance is the length of the shortest path between two points, while Manhattan distance is the sum of the absolute differences of their Cartesian coordinates. The choice between using Euclidean or Manhattan distance in KNN depends on the nature of your data and the problem you are trying to solve."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b54848-f396-49f0-8472-61aaf3b12a9b",
   "metadata": {},
   "source": [
    "# Answer 10:\n",
    "The role of feature scaling in KNN is to prevent features with larger magnitudes from dominating the distance calculations. Feature scaling is a process of normalizing the range of independent variables in the dataset. It is crucial for the KNN algorithm, as it is a distance-based algorithm that relies on measuring distances like euclidean distance between different points. Scaling all features to a common scale gives each feature an equal weight in distance calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff3aa4c-eea9-45ba-be7a-a8f756b778f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
