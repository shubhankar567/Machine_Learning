{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1dcdf80b-3570-4308-85c6-d5d6d32dd64d",
   "metadata": {},
   "source": [
    "# Answer 1:\n",
    "The **Filter method** is a feature selection technique that applies a statistical measure to assign a score to each feature. The features are then ranked by the score and either selected to be kept or removed from the dataset. The methods are often univariate and consider the feature independently, or with regard to the dependent variable.\n",
    "\n",
    "Filter methods are model agnostic, meaning they can be used as an input to any machine learning model. However, one thing to keep in mind is that filter methods do not remove multicollinearity, so you must deal with the multicollinearity of features before training models for your data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2503823a-1ae9-4759-a3b2-83aa1f47fc90",
   "metadata": {},
   "source": [
    "# Answer 2:\n",
    "The **Wrapper method** is another feature selection technique that differs from the Filter method in that it is based on a specific machine learning algorithm that we are trying to fit on a given dataset. It follows a greedy search approach by evaluating all the possible combinations of features against the evaluation criterion.\n",
    "\n",
    "The evaluation criterion is simply the performance measure which depends on the type of problem. For example, for regression, the evaluation criterion can be p-values or R-squared, while for classification, it can be accuracy, precision, recall, or f1-score.\n",
    "\n",
    "One advantage of the Wrapper method is that it gives better performance, but it can be computationally expensive and prone to overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c2c2194-0b8f-4192-a01a-fd178184dc38",
   "metadata": {},
   "source": [
    "# Answer 3:\n",
    "**Embedded methods** are feature selection techniques that blend the feature selection algorithm as part of the learning algorithm, thus having its own built-in feature selection methods. Embedded methods encounter the drawbacks of filter and wrapper methods and merge their advantages.\n",
    "\n",
    "Some common techniques used in Embedded feature selection methods include **LASSO (Least Absolute Shrinkage and Selection Operator)**, which performs both variable selection and regularization at the same time. It is essentially Linear Regression with L1 regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ccef967-46d3-4a40-9d4a-90577881f722",
   "metadata": {},
   "source": [
    "# Answer 4:\n",
    "The **Filter method** is a feature selection technique that is faster and usually the better approach when the number of features is huge. However, it has some drawbacks. For example, it does not remove multicollinearity, which means that it may fail to select the best features.\n",
    "\n",
    "The Filter method looks at individual features for identifying their relative importance and may miss important features that are useful when combined with other features. In addition, the Filter method may face challenges in dealing with more complex issues such as dimensionality, data structures, data format, domain expertsâ€™ availability, data sparsity, and result discrepancies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f533f7-a894-4cc0-867d-c679416f3d70",
   "metadata": {},
   "source": [
    "# Answer 5:\n",
    "The **Filter method** is generally preferred over the **Wrapper method** for feature selection in situations where the number of features is very large. This is because the Filter method is computationally faster and more efficient than the Wrapper method, which can be computationally expensive and time-consuming when dealing with a large number of features.\n",
    "\n",
    "Another advantage of the Filter method is that it is model agnostic, meaning that it can be used as an input to any machine learning model. This makes it a good choice when you want to quickly screen and select relevant features without having to train multiple models.\n",
    "\n",
    "In summary, you would prefer using the Filter method over the Wrapper method for feature selection when dealing with a large number of features, when computational efficiency is a concern, or when you want to quickly screen and select relevant features without having to train multiple models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5fef91b-7625-42db-adc6-5653a8f2aec6",
   "metadata": {},
   "source": [
    "# Answer 6:\n",
    "When using the **Filter Method** for feature selection, we would apply a statistical measure to assign a score to each feature. The features are then ranked by the score and either selected to be kept or removed from the dataset. The methods are often univariate and consider the feature independently, or with regard to the dependent variable.\n",
    "\n",
    "In the case of developing a predictive model for customer churn in a telecom company, we could start by calculating the correlation between each feature and the target variable (churn). Features with a high correlation to the target variable would be considered more relevant and could be selected for inclusion in the model.\n",
    "\n",
    "We could also use other statistical tests such as ANOVA or Chi-Squared to determine the relationship between categorical features and the target variable. Additionally, we could calculate mutual information or use other measures such as Fisher Score or ReliefF to rank the importance of each feature.\n",
    "\n",
    "After ranking the features based on their relevance to the target variable, we would select a subset of the most relevant features to include in your model. It's important to note that Filter methods do not remove multicollinearity, so we may need to deal with multicollinearity of features before training your model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca12aaf-ca30-4356-8d2f-a473068450cc",
   "metadata": {},
   "source": [
    "# Answer 7:\n",
    "When using the **Embedded method** for feature selection, the feature selection algorithm is blended as part of the learning algorithm, thus having its own built-in feature selection methods. Embedded methods encounter the drawbacks of filter and wrapper methods and merge their advantages.\n",
    "\n",
    "In the case of developing a predictive model for the outcome of a soccer match, we would start by selecting a learning algorithm that has built-in feature selection methods, such as LASSO (Least Absolute Shrinkage and Selection Operator) or Decision Trees. These algorithms have mechanisms to automatically select the most relevant features during the model training process.\n",
    "\n",
    "For example, when using LASSO, the algorithm performs both variable selection and regularization at the same time. It is essentially Linear Regression with L1 regularization. The L1 regularization term in the objective function of the algorithm encourages the coefficients of less important features to shrink to zero, effectively removing them from the model.\n",
    "\n",
    "Similarly, when using Decision Trees, the algorithm selects the most relevant features by measuring the importance of each feature in reducing the impurity of the target variable at each split in the tree. Features that are more important in reducing impurity are selected for inclusion in the model.\n",
    "\n",
    "In summary, when using the Embedded method for feature selection in your soccer match prediction project, you would select a learning algorithm with built-in feature selection methods and train your model on the dataset. The algorithm would automatically select the most relevant features during the model training process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e322cdc1-82dd-4e35-a581-29687cd056d7",
   "metadata": {},
   "source": [
    "# Answer 8:\n",
    "When using the **Wrapper method** for feature selection, we would start by selecting a specific machine learning algorithm that we want to use to fit our model. The Wrapper method follows a greedy search approach by evaluating all the possible combinations of features against the evaluation criterion, which is simply the performance measure of the chosen algorithm.\n",
    "\n",
    "In the case of developing a predictive model for house prices, we would start by selecting a performance measure that is appropriate for our problem, such as mean squared error or R-squared. We would then train our model on all possible combinations of features and evaluate the performance of the model using the chosen performance measure.\n",
    "\n",
    "After evaluating all possible combinations of features, we would select the subset of features that resulted in the best performance of our model according to the evaluation criterion. This subset of features would be considered the most relevant for predicting house prices and would be included in our final model.\n",
    "\n",
    "It's important to note that the Wrapper method can be computationally expensive and time-consuming when dealing with a large number of features. However, since we have a limited number of features in this case, the computational cost should be manageable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ea3075-19d4-40b5-bda6-8b696f49e878",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
