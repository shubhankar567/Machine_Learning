{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f84c6ea3-5e72-449b-95f0-9c440434940f",
   "metadata": {},
   "source": [
    "# Answer 1:\n",
    "Lasso Regression is a type of linear regression that uses **shrinkage**. Shrinkage is where data values are shrunk towards a central point, like the mean. The lasso procedure encourages simple, sparse models (i.e. models with fewer parameters). It is a regression analysis method that performs both **variable selection** and **regularization** in order to enhance the prediction accuracy and interpretability of the resulting statistical model.\n",
    "\n",
    "The advantage of lasso regression compared to least squares regression lies in the **bias-variance tradeoff**. The basic idea of lasso regression is to introduce a little bias so that the variance can be substantially reduced, which leads to a lower overall MSE.\n",
    "\n",
    "Lasso regression and ridge regression are both known as regularization methods because they both attempt to minimize the sum of squared residuals (RSS) along with some penalty term. However, Ridge regression only reduces the coefficients close to zero but not zero, whereas Lasso regression can reduce coefficients of some features to zero, thus resulting in better feature selection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc8761b2-dabd-4329-9af9-5cbfba747103",
   "metadata": {},
   "source": [
    "# Answer 2:\n",
    "The main advantage of using Lasso Regression in feature selection is that it has the ability to set the coefficients for features it does not consider interesting to zero. This means that the model does some **automatic feature selection** to decide which features should and should not be included on its own. This method is significant in the minimization of prediction errors that are common in statistical models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31877a7a-5d38-4709-8445-b43f54f3e19e",
   "metadata": {},
   "source": [
    "# Answer 3:\n",
    "The coefficients in a Lasso Regression model represent the average effect on the response variable of a one unit increase in the corresponding predictor variable, holding all other predictors fixed. The interpretation of the coefficients is similar to that of a linear regression model. However, due to the L1 regularization used in Lasso Regression, some coefficients can become zero and eliminated from the model.\n",
    "\n",
    "It is important to note that the magnitude of the coefficients does not necessarily indicate the importance of a feature in prediction. The scale of the predictor variables can affect the magnitude of the coefficients, so it is important to standardize the predictor variables before fitting a Lasso Regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb309b3d-8e26-4635-9b48-118b986625dd",
   "metadata": {},
   "source": [
    "# Answer 4:\n",
    "The main tuning parameter in Lasso Regression is the regularization parameter, denoted by λ (lambda). This parameter controls the strength of the L1 penalty term applied to the coefficients of the model. The value of λ determines the amount of shrinkage applied to the coefficients, where data values are shrunk towards a central point, like the mean.\n",
    "\n",
    "When λ is set to 0, no shrinkage is applied and the Lasso Regression model is equivalent to a linear regression model. As λ increases, more shrinkage is applied and more coefficients are set to zero and eliminated from the model. When λ is very large, all coefficients are set to zero.\n",
    "\n",
    "The value of λ can be chosen using cross-validation or other model selection techniques. The optimal value of λ will depend on the specific data being modeled and the goals of the analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8091781-e48f-477d-ab87-1e8e24b77ec2",
   "metadata": {},
   "source": [
    "# Answer 5:\n",
    "Lasso Regression is primarily designed for linear regression problems, where the relationship between the response variable and the predictor variables is linear. However, it can be extended to handle non-linear relationships by using non-linear transformations of the predictor variables or by using basis expansions, such as polynomial or spline basis functions.\n",
    "\n",
    "In these cases, Lasso Regression can still be used to perform variable selection and regularization by shrinking the coefficients of the transformed or basis-expanded predictor variables towards zero."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "168ffcf5-e6d4-4f0b-866a-28343d461d15",
   "metadata": {},
   "source": [
    "# Answer 6:\n",
    "Ridge Regression and Lasso Regression are both regularization methods that minimize the sum of squared residuals along with some penalty term. The main difference between the two methods lies in the form of the penalty term.\n",
    "\n",
    "In Ridge Regression, the penalty term is the L2 norm of the coefficients, which is the sum of the squares of the coefficients. This penalty term shrinks the coefficients towards zero, but does not set any of them exactly to zero. As a result, Ridge Regression does not perform variable selection and includes all predictor variables in the final model.\n",
    "\n",
    "In contrast, Lasso Regression uses an L1 penalty term, which is the sum of the absolute values of the coefficients. This penalty term can shrink some coefficients all the way to zero, effectively performing variable selection and excluding some predictor variables from the final model.\n",
    "\n",
    "The choice between Ridge Regression and Lasso Regression depends on the specific characteristics of the data being modeled and the goals of the analysis. Ridge Regression may perform better when many predictor variables are significant and have similar coefficients, while Lasso Regression may be more appropriate when only a small number of predictor variables are significant."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea1f653-5b9d-4319-8ab1-a728f275782c",
   "metadata": {},
   "source": [
    "# Answer 7:\n",
    "Yes, Lasso Regression can handle multicollinearity in the input features. Multicollinearity occurs when two or more predictor variables are highly correlated with each other, such that they do not provide unique or independent information in the regression model. This can cause the coefficient estimates of the model to be unreliable and have high variance.\n",
    "\n",
    "Lasso Regression addresses this issue by shrinking the coefficients of the correlated predictor variables towards zero. This has the effect of reducing the impact of the correlated predictor variables on the model, and can help to improve the stability and interpretability of the coefficient estimates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d01e6f45-80f9-4034-acbc-5a445c992af6",
   "metadata": {},
   "source": [
    "# Answer 8:\n",
    "The optimal value of the regularization parameter (lambda) in Lasso Regression is typically chosen using cross-validation or other model selection techniques. One common approach is to fit several Lasso Regression models using different values of lambda and choose the value that produces the lowest test mean squared error (MSE).\n",
    "\n",
    "Cross-validation involves dividing the data into several subsets, fitting the model on a subset of the data, and evaluating its performance on the remaining data. This process is repeated for each subset of the data, and the average performance across all subsets is used to evaluate the model. The value of lambda that produces the best average performance is chosen as the optimal value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71608000-bfaa-49e7-b1b1-86ef9ca463ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
